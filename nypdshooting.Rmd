---
title: "NYPD Shooting Analysis"
author: "Jonathan Lampkin"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  pnypd_data_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Imports, echo=FALSE, include=FALSE}
library(sp)
library(purrr)
library(MASS)
library(spatstat)
library(sf)
library(viridis)
library(ggmap)
library(dplyr)
library(ggplot2)   
library(tidyverse)  
library(lubridate) 
library(car)     
library(caret)
library(pscl)
library(ROCR)
```

```{r Seed}
# Set Seed For Reproducibility
set.seed(42)
```

```{r Load Data, Drop Duplicates, Drop Null Rows, Bin TimeOfDay}
# Load Data, Drop Duplicates and Null Rows
nypd_data <- read_csv("https://data.cityofnewyork.us/api/views/833y-fsy8/rows.csv?accessType=DOWNLOAD") %>%
  distinct() %>%
  drop_na()
```
```{r Structure and Summary}
# Display structure and summary statistics of the data
str(nypd_data)
summary(nypd_data)
head(nypd_data)
```

# Exploratory Data Analysis and Data Cleaning/Transformation

```{r Histogram Value Counts}
# Histogram of Daily Shooting Counts
daily_counts <- nypd_data %>%
  group_by(OCCUR_DATE) %>%
  summarise(SHOOTING_COUNT = n(), .groups = "drop")

ggplot(daily_counts, aes(x = SHOOTING_COUNT)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Shooting Counts per Day",
       x = "Number of Shootings per Day",
       y = "Frequency") +
  theme_minimal()
```

```{r Transformations}
nypd_data <- nypd_data %>%
    mutate(
      OCCUR_DATE = mdy(OCCUR_DATE),
      Year = year(OCCUR_DATE),
      Month = factor(month(OCCUR_DATE, label = TRUE, abbr = TRUE), levels = month.abb),
      DayOfWeek = factor(wday(OCCUR_DATE, label = TRUE, abbr = TRUE), levels = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")),
      TimeOfDay = case_when(
        hour(OCCUR_TIME) >= 6 & hour(OCCUR_TIME) < 12 ~ "Morning",
        hour(OCCUR_TIME) >= 12 & hour(OCCUR_TIME) < 18 ~ "Afternoon",
        hour(OCCUR_TIME) >= 18 & hour(OCCUR_TIME) < 24 ~ "Evening",
        TRUE ~ "Night"
      ))
```

```{r New Structure and Summary}
# Display structure and summary statistics of the data
str(nypd_data)
summary(nypd_data)
head(nypd_data)
```

```{r Aggregated Shooting Counts}
# Count of Shootings Per Year
yearly_counts <- nypd_data %>%
  group_by(Year) %>%
  summarise(SHOOTING_COUNT = n())

# Count of Shootings Per Month
monthly_counts <- nypd_data %>%
  group_by(Month) %>%
  summarise(SHOOTING_COUNT = n())

# Count of Shootings Per Day Of Week
daily_counts <- nypd_data %>%
  group_by(DayOfWeek) %>%
  summarise(SHOOTING_COUNT = n())

# Count of Shootings Per Time Of Day
time_of_day_counts <- nypd_data %>%
  group_by(TimeOfDay) %>%
  summarise(SHOOTING_COUNT = n())

# Count of Shootings Per Borough
borough_counts <- nypd_data %>%
  group_by(BORO) %>%
  summarise(SHOOTING_COUNT = n())


# Yearly Shooting Counts Plot
ggplot(yearly_counts, aes(x = Year, y = SHOOTING_COUNT)) +
  geom_bar(stat = "identity", fill = "blue", color = "black") +
  labs(title = "Yearly Shooting Counts", x = "Year", y = "Shooting Count")

# Monthly Shooting Counts Plot
ggplot(monthly_counts, aes(x = Month, y = SHOOTING_COUNT)) +
  geom_bar(stat = "identity", fill = "yellow", color = "black") +
  labs(title = "Monthly Shooting Counts", x = "Month", y = "Shooting Count")

# Daily Shooting Counts Plot
ggplot(daily_counts, aes(x = DayOfWeek, y = SHOOTING_COUNT)) +
  geom_bar(stat = "identity", fill = "red", color = "black") +
  labs(title = "Daily Shooting Counts", x = "DayOfWeek", y = "Shooting Count")

# Shooting Counts by Time of Day Plot
ggplot(time_of_day_counts, aes(x = TimeOfDay, y = SHOOTING_COUNT)) +
  geom_bar(stat = "identity", fill = "green", color = "black") +
  labs(title = "Shooting Counts by Time of Day", x = "Time of Day", y = "Shooting Count")

# Shooting Counts by Borough Plot
ggplot(borough_counts, aes(x = BORO, y = SHOOTING_COUNT)) +
  geom_bar(stat = "identity", fill = "orange", color = "black") +
  labs(title = "Shooting Counts by Borough", x = "Borough", y = "Shooting Count")
```

```{r Shootings By TimeOfDay and DayOfWeek}
# Shooting counts for each time of day and day of the week
shooting_summary <- nypd_data %>%
  group_by(DayOfWeek, TimeOfDay) %>%
  summarise(ShootingCount = n(), .groups = 'drop')

# Create a heatmap of shootings per time of day and day of week
ggplot(shooting_summary, aes(x = factor(DayOfWeek), y = factor(TimeOfDay), fill = ShootingCount)) +
  geom_tile(color = "white") +
  scale_fill_viridis_c() +
  labs(
    title = "Shooting Counts per Time of Day vs. Day of the Week",
    x = "Day of the Week",
    y = "Time of Day",
    fill = "Shooting Count"
  ) +
  scale_x_discrete(labels = c("1" = "Sunday", "2" = "Monday", "3" = "Tuesday", "4" = "Wednesday",
                              "5" = "Thursday", "6" = "Friday", "7" = "Saturday")) +
  scale_y_discrete(labels = c("1" = "Morning", "2" = "Afternoon", "3" = "Evening", "4" = "Night")) +
  theme_minimal()
```

```{r Shooting Trends By Month and Borough}
# Count of Monthly Shootings by Borough
monthly_borough_trends <- nypd_data %>%
  group_by(Month, BORO) %>%
  summarise(SHOOTING_COUNT = n(), .groups = "drop") # Explicitly drop grouping after summarise

# Plotting monthly shooting counts by borough
ggplot(monthly_borough_trends, aes(x = Month, y = SHOOTING_COUNT, fill = BORO)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Monthly Shooting Counts by Borough", x = "Month", y = "Shooting Counts") +
  scale_fill_brewer(palette = "Set3") +
  theme_minimal()
```


```{r API Key, echo=FALSE}
# Get New York City map
api_key <- "AIzaSyC1Cc8yp4l6R8Cp5ffz1GUvFfNet3mMZtU"
```

```{r New York Map, echo=FALSE}
register_google(key = api_key)

nyc_map <- get_map(location = 'New York City', zoom = 11)
bronx_map <- get_map(location = c(lon = -73.8648, lat = 40.8448), zoom = 13)
brooklyn_map <- get_map(location = c(lon = -73.9442, lat = 40.6782), zoom = 13)

# Plot for New York City
shooting_plot_nyc <- ggmap(nyc_map) +
  geom_point(data = nypd_data, aes(x = Longitude, y = Latitude), 
             color = "red", alpha = 0.25, size = 0.33) + 
  labs(title = "Shooting Incidents in New York City",
       x = "Longitude",
       y = "Latitude") +
  theme_minimal()
print(shooting_plot_nyc)

# Plot for Bronx
shooting_plot_bronx <- ggmap(bronx_map) +
  geom_point(data = nypd_data %>% filter(BORO == "BRONX"), aes(x = Longitude, y = Latitude), 
             color = "red", alpha = 0.5, size = 2) +
  labs(title = "Shooting Incidents in the Bronx",
       x = "Longitude",
       y = "Latitude") +
  theme_minimal()
print(shooting_plot_bronx)

# Plot for Brooklyn
shooting_plot_brooklyn <- ggmap(brooklyn_map) +
  geom_point(data = nypd_data %>% filter(BORO == "BROOKLYN"), aes(x = Longitude, y = Latitude), 
             color = "red", alpha = 0.5, size = 2) +
  labs(title = "Shooting Incidents in Brooklyn",
       x = "Longitude",
       y = "Latitude") +
  theme_minimal()
print(shooting_plot_brooklyn)

# Bronx Heatmap
heatmap_bronx <- ggmap(bronx_map) +
  stat_density2d(data = nypd_data %>% filter(BORO == "BRONX"), 
                 aes(x = Longitude, y = Latitude, fill = ..level..),
                 geom = "polygon", alpha = 0.18) +
  scale_fill_viridis_c() +
  labs(title = "Heatmap of Shooting Incidents in the Bronx",
       x = "Longitude",
       y = "Latitude") +
  theme_minimal()
print(heatmap_bronx)

# Brooklyn Heatmap
heatmap_brooklyn <- ggmap(brooklyn_map) +
  stat_density2d(data = nypd_data %>% filter(BORO == "BROOKLYN"), 
                 aes(x = Longitude, y = Latitude, fill = ..level..),
                 geom = "polygon", alpha = 0.18) +
  scale_fill_viridis_c() +
  labs(title = "Heatmap of Shooting Incidents in Brooklyn",
       x = "Longitude",
       y = "Latitude") +
  theme_minimal()
print(heatmap_brooklyn)
```


```{r Filling in Dataset with Zero Shootings}

# Define the most densely populated coordinates for each borough
densest_coordinates <- list(
  "MANHATTAN" = c(-73.9851, 40.7580),
  "BROOKLYN" = c(-73.9496, 40.6501),
  "QUEENS" = c(-73.8317, 40.7282),
  "BRONX" = c(-73.8648, 40.8448),
  "STATEN ISLAND" = c(-74.1502, 40.5795)
)

# Function to fill in zero-shooting rows for each unique combination of OCCUR_DATE and TimeOfDay
fill_zero_shooting_rows <- function(df, borough_name) {
  full_dates <- seq(min(df$OCCUR_DATE, na.rm = TRUE), max(df$OCCUR_DATE, na.rm = TRUE), by = "day")
  time_of_day_levels <- c("Morning", "Afternoon", "Evening", "Night")
  
  zero_shooting_df <- expand.grid(
    OCCUR_DATE = full_dates,
    TimeOfDay = time_of_day_levels
  ) %>%
    mutate(
      BORO = borough_name,
      Year = year(OCCUR_DATE),
      Month = factor(month(OCCUR_DATE, label = TRUE, abbr = TRUE), levels = month.abb),
      DayOfWeek = factor(wday(OCCUR_DATE, label = TRUE, abbr = TRUE), levels = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")),
      SHOOTING_COUNT = 0,
      Longitude = densest_coordinates[[borough_name]][1],
      Latitude = densest_coordinates[[borough_name]][2]
    )
  
  # Merge with original data and fill missing combinations with zero-shooting rows
  combined_df <- full_join(df, zero_shooting_df, by = c("OCCUR_DATE", "TimeOfDay", "BORO", "Year", "Month", "DayOfWeek", "Longitude", "Latitude"))
  combined_df <- combined_df %>%
    mutate(SHOOTING_COUNT = ifelse(is.na(SHOOTING_COUNT.x), SHOOTING_COUNT.y, SHOOTING_COUNT.x)) %>%
    select(-c(SHOOTING_COUNT.x, SHOOTING_COUNT.y)) %>%
    arrange(OCCUR_DATE, TimeOfDay)
  
  return(combined_df)
}

# Main function to process each borough without aggregating data
process_borough_data <- function(df, borough_name) {
  shooting_df <- df %>%
    filter(BORO == borough_name)
  
  if (!"SHOOTING_COUNT" %in% colnames(shooting_df)) {
    shooting_df$SHOOTING_COUNT <- 1  # Default to 1 for rows indicating shootings
  }
  
  expanded_df <- fill_zero_shooting_rows(shooting_df, borough_name)
  
  expanded_df <- expanded_df %>%
    mutate(
      Year_Binary = ifelse(Year == min(Year, na.rm = TRUE), 0, 1),
      TimeOfDay_Num = as.numeric(factor(TimeOfDay, levels = c("Morning", "Afternoon", "Evening", "Night"))),
      TimeOfDay_Sin = sin(2 * pi * TimeOfDay_Num / 4),
      TimeOfDay_Cos = cos(2 * pi * TimeOfDay_Num / 4),
      Month_Num = as.numeric(Month),
      Month_Sin = sin(2 * pi * Month_Num / 12),
      Month_Cos = cos(2 * pi * Month_Num / 12),
      DayOfWeek_Num = as.numeric(DayOfWeek),
      DayOfWeek_Sin = sin(2 * pi * DayOfWeek_Num / 7),
      DayOfWeek_Cos = cos(2 * pi * DayOfWeek_Num / 7)
    )
  
  return(expanded_df)
}

# Plot the distribution of SHOOTING_COUNT for each borough
plot_shooting_count_distribution <- function(borough_data, borough_name) {
  ggplot(borough_data, aes(x = SHOOTING_COUNT)) +
    geom_histogram(binwidth = 1, fill = "blue", color = "black", alpha = 0.7) +
    labs(
      title = paste("Distribution of SHOOTING_COUNT in", borough_name),
      x = "Shooting Count",
      y = "Frequency"
    ) +
    theme_minimal()
}

# Process each borough and store results without aggregation
borough_datasets <- lapply(names(densest_coordinates), function(borough) {
  process_borough_data(nypd_data, borough)
})
names(borough_datasets) <- names(densest_coordinates)

# Display the head of each borough's dataset to ensure it's ordered by day and time of day
for (borough in names(borough_datasets)) {
  cat("\nShowing head of the dataset for", borough, "ordered by day and time of day:\n")
  ordered_df <- borough_datasets[[borough]] %>%
    arrange(OCCUR_DATE, TimeOfDay)
  print(head(ordered_df, 10))
  df <- borough_datasets[[borough]]
  cat("\nPlotting distribution for", borough, ":\n")
  print(plot_shooting_count_distribution(df, borough))

}


```


```{r Data Preparation}

# Function to select relevant columns
select_relevant_features <- function(df) {
  selected_df <- df %>%
    select(
      Latitude, Longitude, SHOOTING_COUNT, TimeOfDay_Sin, TimeOfDay_Cos, 
      Month_Sin, Month_Cos, DayOfWeek_Sin, DayOfWeek_Cos
    ) %>%
    drop_na()
  return(selected_df)
}

# Function to add interaction and lagged features
add_features <- function(df) {
  if ("TimeOfDay_Sin" %in% colnames(df) && "DayOfWeek_Sin" %in% colnames(df)) {
    # Create a numeric interaction by multiplying TimeOfDay_Sin and DayOfWeek_Sin
    df <- df %>%
      mutate(TimeOfDay_DayOfWeek_Interaction = TimeOfDay_Sin * DayOfWeek_Sin)
  }
  return(df)
}
```

```{r Data Splitting and Scaling}
# Function to split data
split_train_test <- function(df, train_ratio = 0.8) {
  # Create a random sample of indices for the training data
  train_indices <- sample(1:nrow(df), size = floor(train_ratio * nrow(df)))
  
  # Training set
  x_train <- df[train_indices, ] %>% select(-SHOOTING_COUNT)
  y_train <- df[train_indices, "SHOOTING_COUNT", drop = TRUE]
  
  # Testing set (the rest of the data not in the training set)
  x_test <- df[-train_indices, ] %>% select(-SHOOTING_COUNT)
  y_test <- df[-train_indices, "SHOOTING_COUNT", drop = TRUE]

  # Return a list of train and test data
  return(list(x_train = x_train, y_train = y_train, x_test = x_test, y_test = y_test))
}


# Function to scale data
scale_data <- function(x_train, x_test) {
  # Calculate the mean and standard deviation of the training data
  train_mean <- colMeans(x_train)
  train_std <- apply(x_train, 2, sd)
  
  # Scale the training data
  x_train_scaled <- scale(x_train, center = train_mean, scale = train_std)
  
  # Scale the test data using the same mean and standard deviation as the training data
  x_test_scaled <- scale(x_test, center = train_mean, scale = train_std)
  
  return(list(x_train_scaled = x_train_scaled, x_test_scaled = x_test_scaled))
}
```

```{r Model Training and Prediction, echo=FALSE}
# Function to train and evaluate models with added checks
train_predict_models <- function(x_train, y_train, x_test, y_test) {
  
  # Ensure y_train and y_test are vectors
  y_train <- as.vector(y_train)
  y_test <- as.vector(y_test)
  
  # Create a data frame for training data
  train_data <- data.frame(SHOOTING_COUNT = y_train, x_train)
  test_data <- data.frame(SHOOTING_COUNT = y_test, x_test)
  
  # Logistic regression model
  full_logistic_model <- suppressWarnings(glm(SHOOTING_COUNT ~ ., family = binomial(link = "logit"), data = train_data))
  
  # Print AIC and BIC of the full model
  cat("\nFull Model AIC:", AIC(full_logistic_model))
  cat("\nFull Model BIC:", BIC(full_logistic_model), "\n")
  
  # Check VIF for multicollinearity
  cat("\nCalculating VIF for Multicollinearity:\n")
  vif_values <- vif(full_logistic_model)
  print(vif_values)

  # Run ANOVA analysis
  cat("\nANOVA Analysis:\n")
  anova_results <- suppressWarnings(anova(full_logistic_model, test = "Chisq"))
  print(anova_results)
  
  # Backward feature selection
  cat("\nPerforming Backward Feature Selection...\n")
  selected_model <- suppressWarnings(step(full_logistic_model, direction = "backward", trace = 0))
  
  # Print AIC and BIC of the selected model
  cat("\nSelected Model AIC:", AIC(selected_model))
  cat("\nSelected Model BIC:", BIC(selected_model), "\n")
  
  # Print the summary of the selected model
  cat("\nSelected Model Summary:\n")
  print(summary(selected_model))  

  
  # Use summary to get detailed insight into the model coefficients
  cat("\nModel Summary:\n")
  print(summary(selected_model))
  
  # Generate predictions
  train_pred_prob <- predict(selected_model, newdata = train_data, type = "response")
  test_pred_prob <- predict(selected_model, newdata = test_data, type = "response")

  # Convert probabilities to binary predictions
  train_pred_binary <- as.vector(ifelse(train_pred_prob > 0.5, 1, 0))
  test_pred_binary <- as.vector(ifelse(test_pred_prob > 0.5, 1, 0))
  
  return(list(train_pred_binary = train_pred_binary, test_pred_binary = test_pred_binary, test_pred_prob = test_pred_prob))
}
```

```{r Metrics Calculation and ROC Plotting}
calculate_classification_metrics <- function(y_true, y_pred) {
  confusion <- confusionMatrix(y_pred, y_true)
  accuracy <- confusion$overall['Accuracy']
  precision <- confusion$byClass['Pos Pred Value']
  recall <- confusion$byClass['Sensitivity']
  f1_score <- 2 * ((precision * recall) / (precision + recall))
  
  data.frame(
    Accuracy = accuracy,
    Precision = precision,
    Recall = recall,
    F1_Score = f1_score
  )
}

# Function to plot ROC curve using ROCR
plot_roc_curve <- function(y_test, test_pred_prob, title = "ROC Curve") {
  # Ensure y_test and test_pred_prob are numeric
  y_test <- as.numeric(as.character(y_test))
  test_pred_prob <- as.numeric(test_pred_prob)
  
  # Create ROCR prediction and performance objects
  pred <- prediction(test_pred_prob, y_test)
  perf <- performance(pred, "tpr", "fpr")
  
  # Plot the ROC curve
  plot(perf, col = "blue", main = title)
  auc <- performance(pred, "auc")
  auc_value <- auc@y.values[[1]]
  legend("bottomright", legend = paste("AUC =", round(auc_value, 3)), col = "blue", lwd = 2)
}


plot_actual_vs_predicted <- function(y_true, y_pred, borough, title = "Actual vs Predicted") {
  # Create a confusion matrix table
  confusion_table <- table(Predicted = y_pred, Actual = y_true)
  
  # Plot the confusion matrix as a mosaic plot
  mosaicplot(
    confusion_table,
    main = paste(borough, ":", title),
    xlab = "Actual",
    ylab = "Predicted",
    color = TRUE,
    las = 1
  )
}


# After training your model and generating predictions:
evaluate_models <- function(y_true, y_pred, y_pred_prob, borough) {
  # Calculate and print metrics with borough name
  cat("\nMetrics for", borough, ":\n")
  metrics <- calculate_classification_metrics(y_true, y_pred)
  print(metrics)
  
  # Plot actual vs predicted with borough label
  plot_actual_vs_predicted(y_true, y_pred, borough, "Logistic Model: Actual vs Predicted")
  
  # Plot ROC curve with `ROCR`
  plot_roc_curve(y_true, y_pred_prob, paste(borough, ": Logistic Model ROC Curve"))
}

```

```{r Loop Through Boroughs}
# Apply the process for each borough with added checks
for (boro in names(borough_datasets)) {
  cat("\nProcessing dataset for", boro, ":\n")
  df <- borough_datasets[[boro]]

  # Preprocess data
  df <- select_relevant_features(df)
  df <- add_features(df)

  # Split and scale data
  split_data <- split_train_test(df)
  scaled_data <- scale_data(split_data$x_train, split_data$x_test)

  preds <- train_predict_models(scaled_data$x_train_scaled, split_data$y_train, scaled_data$x_test_scaled, split_data$y_test)

  # Ensure y_test and y_pred are vectors
  y_test_vector <- factor(split_data$y_test)
  y_pred_vector <- factor(preds$test_pred_binary)
  
  # Align levels between y_test_vector and y_pred_vector
  common_levels <- union(levels(y_test_vector), levels(y_pred_vector))
  y_test_vector <- factor(y_test_vector, levels = common_levels)
  y_pred_vector <- factor(y_pred_vector, levels = common_levels)
  
  # Keep the probabilities as numeric
  y_pred_prob_vector <- preds$test_pred_prob
  
  # Evaluate models with borough name included
  eval <- evaluate_models(y_test_vector, y_pred_vector, y_pred_prob_vector, boro)
}
```

# Bias Analysis
1. Reporting Bias
This dataset only includes shootings recorded by the NYPD. Therefore, unreported incidents or errors in recording can lead to an underestimation or misrepresentation in the shooting data. This type of reporting bias may skew our results, especially if certain areas or types of incidents are underreported.

2. Geographic Bias
The data is collected solely from New York City and broken down by borough, so any analysis is specific to this location. Trends identified here may not be generalizable to other cities or regions. Additionally, some boroughs may have more densely populated areas, which can lead to higher numbers of reported shootings simply due to higher population density, rather than increased per capita shooting rates.

3. Temporal Bias
The dataset only includes data from 2022 and 2023. This is a relatively short time frame and may not capture longer-term trends, seasonal patterns, or year-to-year variations. The limited period could lead to incorrect assumptions about trends if we generalize beyond this timeframe.

4. Analytical Bias
In the modeling section, we used features such as Year, Month, DayOfWeek, TimeOfDay, and BORO. If these features are not representative of the true underlying causes of shooting incidents, the model might overfit to patterns that do not generalize well. Furthermore, the model assumes a Binomial distribution, which might not fully capture the variance in the data. Alternatively, aggregating this data would have allowed for a poisson based model where the data can range from 0 to infinity (theoretically).

5. Analysis Interpretation Bias
The way we interpret the findings from our visualizations and model predictions can introduce bias. For example, higher predicted shooting counts in specific boroughs might be attributed to socio-economic factors, which we haven't analyzed here. Such assumptions could lead to incorrect or overly simplified conclusions.

6. Unaccounted for Variable Bias
Policing policies or socio-economic factors might influence shooting counts but are not part of this dataset.

# Model Selection

The Logistic regression model is well-suited for classification tasks and is easily interpretable. 
In the future we may choose to model shooting counts as a poisson random variable by summing shooting counts over time periods which can provide a different insight.

# Interpretation of Results
The results from our analysis suggest several notable trends:

Temporal Trends: Our visualizations show differences in shooting counts by month, time of day, and year.

Spatial Trends: Shootings stratified by boroughs revealed that certain boroughs experience notably higher shooting counts, with the Bronx and Brooklyn having more incidents than others. Additionally, The coordinates consistently showed statistical significance in our ANOVA tests and in our model summary.

Model Findings: Predictor significance varied by borough, but notably longitude and latitude remained consistent statistically significant predictors. In boroughs with more sparsely distributed shootings, through backward feature selection, the model found longitude and latitude to be the only statistically significant predictors. In boroughs with more shooting incidents, features such as time of day, day of week, and month of year also became significant.


These findings can support decision-makers in identifying higher-risk periods and areas for shootings. However, caution should be taken due to potential biases and the model's limitations in capturing complex causative factors.